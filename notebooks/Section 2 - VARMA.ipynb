{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Project Research Notebook\n",
    "\n",
    "---\n",
    "## 1.0: Data Processing\n",
    "\n",
    "### 1.1: Retrieving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "//\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "//\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ac140098a7e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_hashtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtslib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tslib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# hack but overkill to use re\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m from pandas._libs.tslibs import (\n\u001b[0;32m     15\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\interval.pyx\u001b[0m in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import urllib\n",
    "import datetime\n",
    "import csv\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from datetime import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# https://stackoverflow.com/questions/12433076/download-history-stock-prices-automatically-from-yahoo-finance-in-python\n",
    "# Gets data for list of tickersm from start date to end date, saves in path.\n",
    "def get_stocks(tickers, start, end, path):\n",
    "    \n",
    "    #Converts a timestring in format 'Y-m-d' UNIX timestamp as a string\n",
    "    def convert_timestring(time_string):\n",
    "        return int(time.mktime(dt.strptime(time_string, '%Y-%m-%d').timetuple()))\n",
    "\n",
    "    #Returns a url for the data between 'start time' and 'end time' for 'ticker'\n",
    "    def make_url(ticker_symbol, start, end):\n",
    "        #Convert datetime to UNIX timestamp\n",
    "        start = convert_timestring(start)\n",
    "        end = convert_timestring(end)\n",
    "        url = 'https://query1.finance.yahoo.com/v7/finance/download/'+ ticker_symbol + '?period1=' + str(start) + '&period2=' + str(end) + '&interval=1d&events=history&crumb=jIa1MqK//LA'        \n",
    "        return url\n",
    "    \n",
    "    #Creates fullpath for the file, naming data as the ticke\n",
    "    def make_filename(ticker, path):\n",
    "        return path + \"/\" + ticker + \".csv\"\n",
    "    \n",
    "    #Gets the data for the specified time and tickers and save them to file\n",
    "    def pull_historical_data(tickers):\n",
    "        filename = None\n",
    "        url = None\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            filename = make_filename(ticker, path)\n",
    "            url = make_url(ticker, start, end)\n",
    "\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, filename)\n",
    "            except urllib.request.ContentTooShortError as e:\n",
    "                outfile = open(filename, \"w\")\n",
    "                outfile.write(e.content)\n",
    "                outfile.close()\n",
    "            \n",
    "            print('Written file:' + filename)\n",
    "\n",
    "    #Exicute code\n",
    "    pull_historical_data(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading csv files into pandas returns a dictionary with dataframes of tickers\n",
    "def load_to_df(tickers, path):\n",
    "    data = {}\n",
    "    \n",
    "    for i in range(len(tickers)):\n",
    "        full_path = path + \"/\" + tickers[i] + \".csv\"\n",
    "        ticker = tickers[i].replace('^', '')\n",
    "        data.update({ticker: pd.read_csv(full_path, parse_dates=True, index_col=0)})\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downlaoding Files to .csv\n",
    "path = \"C:\\\\Users\\\\noahd\\\\Google Drive\\\\University\\\\2k20-21\\\\Personal Project\\\\Data\\\\Stocks\"\n",
    "tickers = [\"^GSPC\", \"^DJI\", \"^FTSE\", \"^N225\", \"^BSESN\"]\n",
    "dates = ('2011-04-07', '2021-04-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stocks(tickers, dates[0], dates[1], path)\n",
    "data_raw = load_to_df(tickers, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplfinance as mpf\n",
    "from datetime import datetime\n",
    "\n",
    "#Takes a list of series to plot, attirbutes to plot, and time span\n",
    "def plot_stock_series(series_dict, start=0, end=-1):\n",
    "    \n",
    "    for key, value in series_dict.items():\n",
    "        mpf.plot(value[start: end], figratio=(9, 3), mav=(20),volume=True,title = key.upper(),tight_layout=True, style=\"binance\",type=\"ohlc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting all data\n",
    "plot_stock_series(data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Normalising and Differencing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#Applies normalisation and difference transfrom to a pandas dataframe\n",
    "#Can only normalise and remove Na if passed diff=False\n",
    "def process_data(data, dates, diff=True):\n",
    "    data_proc = data.copy()\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "    for key in data_proc:\n",
    "        df_copy = data_proc[key].copy(deep=True)# So the original data is not overwritten\n",
    "        df_copy = df_copy.reindex(pd.date_range(start=dates[0], end=dates[1], freq='D')) #For using mutiple time series, all trackers must have the same dates\n",
    "        \n",
    "        if diff==True:\n",
    "            df_copy = df_copy.diff() #Difference transform\n",
    "        \n",
    "        df_copy = df_copy.resample('d').mean().interpolate('spline', order=3, s=0) #Iterpolate missing dates\n",
    "        df_copy = pd.DataFrame(scaler.fit_transform(df_copy.values), columns = df_copy.columns, index=df_copy.index.values) #Scales values\n",
    "        df_copy = df_copy.drop(df_copy.tail(1).index)\n",
    "        df_copy = df_copy.dropna(axis=0)\n",
    "        data_proc[key] = df_copy # Assign new data back to data dictionary    \n",
    "    \n",
    "    return data_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the statisical model used, the data might need to be differenced to remove a trend, or only normalised. Datasets of both will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Processing all Datasets\n",
    "data_normal = process_data(data_raw, dates, diff=False)\n",
    "data_diff = process_data(data_raw, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_series(data_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_series(data_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Data Exploration\n",
    "#### 1.21: Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "#Decomposes \n",
    "def decompose_trend(data, plot=True):\n",
    "    \n",
    "    data_decomp = dict.fromkeys(data.keys(),{}) #Initialise dictionary for data decompostion result\n",
    "    kwargs = {'linewidth': 0.2} #.plot kwargs\n",
    "    colours = ['black', 'green', 'blue', 'cyan', 'magenta', 'red']\n",
    "    \n",
    "    for key, value, in data.items():\n",
    "        for column in data[key].columns:\n",
    "            data_decomp[key].update({column: sm.tsa.seasonal_decompose(data[key][column].dropna(),period=365)})\n",
    "            \n",
    "        if plot==True:\n",
    "            fig, axes = plt.subplots(ncols=4, nrows=len(data[key].columns), sharex=True, figsize=(12,5))\n",
    "            for i, column in enumerate(data_decomp[key]):\n",
    "                fig.suptitle(key)\n",
    "                data_decomp[key][column].observed.plot(ax=axes[i][0], **kwargs)\n",
    "                data_decomp[key][column].trend.plot(ax=axes[i][1], **kwargs)\n",
    "                data_decomp[key][column].seasonal.plot(ax=axes[i][2], **kwargs)\n",
    "                data_decomp[key][column].resid.plot(ax=axes[i][3], **kwargs)\n",
    "                axes[i][0].set_xlabel('Observed', fontsize = 7) # Y label\n",
    "                axes[i][0].set_ylabel(column, fontsize = 7) # Y label\n",
    "                axes[i][0].tick_params(axis='both', labelsize=7)\n",
    "                axes[i][1].tick_params(axis='both', labelsize=7)\n",
    "                axes[i][2].tick_params(axis='both', labelsize=7)\n",
    "                axes[i][3].tick_params(axis='both', labelsize=7)\n",
    "                axes[i][0].get_lines()[0].set_color(colours[i])\n",
    "                axes[i][1].get_lines()[0].set_color(colours[i])\n",
    "                axes[i][2].get_lines()[0].set_color(colours[i])\n",
    "                axes[i][3].get_lines()[0].set_color(colours[i])\n",
    "                \n",
    "                \n",
    "            plt.show()\n",
    "    \n",
    "    return data_decomp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_decomp = decompose_trend(data_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.22: Auto Correlation Plot\n",
    "Auto Correlationplots can help determine information about the type of statistical model best suited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "def plot_autocorrelation(data):\n",
    "    \n",
    "    for key, df in data.items():\n",
    "        for column in df.columns:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12,3))\n",
    "            plot_acf(df[column], ax=axes[0], title=key+' '+column+' - Autocorrelation')\n",
    "            plot_pacf(df[column], ax=axes[1], title=key+' '+column+' - Partial Autocorrelation')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorrelation(data_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The all of the data has very weak AC and PAC, with the exception of the volume, which normaly has weak negative correlation in the first few lags. This outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.13: ADF Test\n",
    "Augmented Dickey–Fuller (ADF) test can be used to determine if a series is stationary or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#Calculates the ADF results for full dataset orginal and differenced, returns a dataframe with results.\n",
    "def batch_adf(orginal, diff, significance, start=0, end=-1):\n",
    "    \n",
    "    def analyse_res(adf_res, significance):\n",
    "        if adf_res[0] < significance:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    #contstruct results table with multi index for orginal and diff\n",
    "    df_index = [[], []]\n",
    "    \n",
    "    for i, key in enumerate(orginal.keys()):\n",
    "        df_index[0].extend([key, key])\n",
    "        df_index[1].extend(['Orig', 'Diff'])\n",
    "    \n",
    "    analysis = pd.DataFrame(index=df_index, columns=orginal[key].columns)\n",
    "    \n",
    "    # Get ADF test results and insert to table\n",
    "    for (key_o, value_o), (key_d, value_d) in zip(orginal.items(), diff.items()):\n",
    "        for (column_o, column_d) in zip(value_d.columns, value_d.columns):\n",
    "            analysis.loc[(key_o, 'Orig'), column_o] = analyse_res(adfuller(value_o[column_o], regression='c', autolag='AIC'), significance)\n",
    "            analysis.loc[(key_d, 'Diff'), column_d] = analyse_res(adfuller(value_d[column_d], regression='c', autolag='AIC'), significance)\n",
    "\n",
    "    \n",
    "    return analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_res_p005 = batch_adf(data_normal, data_diff, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table of Results ADF Test; Critical = 0.05\n",
    "False: ADF p-value higher than critical\n",
    "\n",
    "True: ADF p-value lower than critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_res_p005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results show that a portion of the un-differenced data has a trend, based on a critical p-value of 0.05. Whist the differenced data had no trend. The mixed results for the undifferenced data having mixed results is suprising for series falling under; Open, High, Low, Close, Adjusted Close, for tickers; FSTE, N255. Since these series had visible upward trends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2:ARIMAX as Baseline\n",
    "\n",
    "A statisical model will be imployed as a baseline comparison to test a machine learning based model against. For the mdoel VAR (Vector AutoRegression) model will be used. Since the goal of the reaserch is to test a machine learning model using multiple time series in conjuction to forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: ARIMAX Configuration\n",
    "For this test the S&P 500 close series will be used, only half of the data will be used as the dataset is very large. The data will be split into testing and training with a 7:3 train:test split. In the context of this reaserch the model will be used for walk forward prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes data dictionary and creates a 2d numpy array where all time series have the same level\n",
    "def flatten_dataset(data_dict, n_shifts):\n",
    "    \n",
    "    x = []\n",
    "\n",
    "    for i, df in enumerate(data_dict.values()):\n",
    "        data = list(list(x) for x in zip(*(df[x].values.tolist() for x in df.columns)))\n",
    "        x.append(data)\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    x  = np.swapaxes(x, 0, 1)\n",
    "    x = x.reshape(3651, 30)\n",
    "    y = np.roll(x, n_shifts, axis=0)\n",
    "    print(x.shape, y.shape)    \n",
    "    \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varmax_x, varmax_y = flatten_dataset(data_diff, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rolling Forecast Implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning #Ignores warnings\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "import copy\n",
    "\n",
    "# Trains data with rolling forecasting, each sample is predicted based on all previous in chronological order\n",
    "# order is a tuple with the p, d and q values\n",
    "def varmax_rolling(train, test, order):\n",
    "    x = copy.deepcopy(train) # Rolling prediction\n",
    "    pred = list()\n",
    "    \n",
    "    #Rolling forecast\n",
    "    for row in test:\n",
    "        model = VARMAX(x, order=order, error_cov_type='diagonal') #Builds the model using the training set and previous test values\n",
    "        fitted_model = model.fit()\n",
    "        pred.append(fitted_model.forecast()[0]) #Collects the predicted values for next data point\n",
    "        x.append(row) #Adds the next data point from test\n",
    "        print(row)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing functions on years worth of data from S&P500/GSPC open value\n",
    "train, test = train_test_split(varmax_x[0 : 28], test_size=0.3, shuffle=False)\n",
    "pred = varmax_rolling(train, test, (5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA results plotting\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.metrics import classification_report as clf_report\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_varmax_res(y, pred):\n",
    "    \n",
    "    #Finds the percentage error for each value\n",
    "    def p_error(y, pred):\n",
    "        return pd.Series((abs(pred.values - y.values)/y.values) *100, y.index)\n",
    "    \n",
    "    error = p_error(y, pred)\n",
    "    fig = plt.figure(figsize=(14, 3))\n",
    "    red_patch = mpatches.Patch(color='red', label='Predicted')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='True')\n",
    "    green_patch = mpatches.Patch(color='green', label='Pecentage Error')\n",
    "\n",
    "    fig.legend(handles=[red_patch, blue_patch, green_patch], ncol=3, loc='upper center')\n",
    "    ax = y.plot(color=\"blue\")\n",
    "    pred.plot(ax=ax, color=\"red\")\n",
    "    ax.set_ylabel('Normalised Value', color='black')\n",
    "    ax.legend=['Test Data', 'Predicted']\n",
    "    ax2=ax.twinx()\n",
    "    error.plot(ax=ax2,color=\"green\",linestyle=\"--\")\n",
    "    ax2.set_ylabel('Percentage Error', color='green')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: ARIMAX Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: ARIMAX Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search  Tuning\n",
    "For every combination of (p, d, q), within a difined search space the ARIMA will be run using rolling/walk forward predictions, the MSE between each model will be compared to find the closest fitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the data to fit and validate, and an object (search_space) and trails all combinations of ARIMA orders.\n",
    "# Returns the order of the best fitting model in terms of MSE\n",
    "def grid_search_ARIMA(series, search_space, test_size=0.3):\n",
    "    train, test = train_test_split(series, test_size=test_size)\n",
    "    search_history = []\n",
    "    \n",
    "    for p in search_space['p']:\n",
    "        for d in search_space['d']:\n",
    "            for q in search_space['q']:\n",
    "                arima_order = (p, d, q)\n",
    "                pred = arima_rolling_forecast(train, test, arima_order)\n",
    "                error = mape(pred, test)\n",
    "                search_history.append((arima_order, error))\n",
    "                print('Order: ' + str(arima_order) + '; MAPE: ' + str(error))\n",
    "    \n",
    "    search_history = sorted(search_history, key=lambda x:x[1])\n",
    "    print('Best Order:' + str(search_history[0]))\n",
    "    #print('Done')\n",
    "    \n",
    "    return search_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {'p': list(range(1, 8)),\n",
    "                'd': list(range(0, 2)),\n",
    "                'q': list(range(0, 3))}\n",
    "gs_test = grid_search_ARIMA(data_diff['GSPC']['Close']['2011-04-01':'2011-06-01'], search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 ARIMAX Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.41 ARIMAX Validation Plenimary Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3: Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: CNN-LSTM Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9e709ef071b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkerastuner\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Model Layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtraining_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetwork_serialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompile_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloss_scale_optimizer\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlso\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m   \u001b[0mscipy_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m   \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m   \u001b[0mpd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_hashtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtslib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tslib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# hack but overkill to use re\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m from pandas._libs.tslibs import (\n\u001b[0;32m     15\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\interval.pyx\u001b[0m in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "import keras as kr\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "#Model Layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://gist.github.com/GermanCM/1943a0dc1eac04f848c6fe9b16947ac4\n",
    "#Contains methods for building, trainig, optimising and validating the model.\n",
    "        \n",
    "    \n",
    "#Creates a 3D vector, where the date is along the x, y and z are the tracker and tracker-attrribute.\n",
    "#Returns this data format, and a shifted version to use for rolling predition as y\n",
    "def format_data_cnn(data_dict):\n",
    "    data = []\n",
    "    \n",
    "    for i, df in enumerate(data_dict.values()):\n",
    "        data.append(list(list(x) for x in zip(*(df[x].values.tolist() for x in df.columns))))\n",
    "    \n",
    "    data = np.asarray(data)\n",
    "    data  = np.swapaxes(data, 0, 1) #Get the data in right alignment\n",
    "    data = np.reshape(data, (data.shape[0], (data.shape[1]*data.shape[2])), order='C') #Merge 2nd and 3d axis to make 2D array\n",
    "    print(data.shape)\n",
    "\n",
    "    return data\n",
    "\n",
    "def reset_weights(model):\n",
    "    import keras.backend as K\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'): \n",
    "            layer.kernel.initializer.run(session=session)\n",
    "        if hasattr(layer, 'bias_initializer'):\n",
    "            layer.bias.initializer.run(session=session)  \n",
    "\n",
    "\n",
    "\n",
    "#Takes hyperpameter tuner as input reutns model, to be run with keras tuner\n",
    "def model_builder(hp, time_steps=1, features=30, params='Egg'):\n",
    "    model = Sequential()\n",
    "\n",
    "    #Dictionary organising all params for easy reading\n",
    "    params = {\n",
    "        # Layer Parameters\n",
    "        'conv1d' : {\n",
    "            'units' : hp.Int('conv_units', min_value=1, max_value=8, step=1), #Conv1D Units\n",
    "            'kernal_size' : hp.Int('kernal_size', min_value=2, max_value=10, step=1),\n",
    "            'filters' : hp.Int('filters', min_value=4, max_value=16, step=1)\n",
    "        },\n",
    "        'max_pooling' : {\n",
    "            'pool_size' : hp.Int('pool_size', min_value=1, max_value=4, step=1)\n",
    "        },\n",
    "        'lstm' : {\n",
    "            'units' : hp.Int('lstm_units', min_value=5, max_value=255, step=5)\n",
    "        },\n",
    "        'dense' : {\n",
    "            'units' : hp.Int('dense_units', min_value=4, max_value=256, step=4)\n",
    "        },\n",
    "        # Hyper Parameters\n",
    "        'hp' : {\n",
    "            'lr' : hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    if(params=='Default'):\n",
    "        paramas = {\n",
    "        'conv1D' : {'units': 1, 'filters' : 7},\n",
    "        'maxpooling' : {'pool_size' : 2},\n",
    "        'ltsm': {'units': 45},\n",
    "        'dense': {'units': 128},\n",
    "        'hp': {'lr': 1e-2}\n",
    "    }\n",
    "\n",
    "    # Add time distributed -wrapped CNN layers\n",
    "    model.add(Conv1D(filters=params['conv1d']['filters'],kernel_size=params['conv1d']['kernal_size'],\n",
    "        activation='relu', \n",
    "        padding='causal',\n",
    "        input_shape=(time_steps,features)))\n",
    "    model.add(MaxPooling1D(pool_size=params['max_pooling']['pool_size'], padding='valid'))\n",
    "    model.add(LSTM(params['lstm']['units'])) #LTSM Layer\n",
    "    model.add(Dense(params['dense']['units']))\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    #Compile Model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=params['hp']['lr']),\n",
    "        loss=kr.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "#Fits the model using keras tuner, hypertuner object. Returns the most model.\n",
    "def fit_model(train, tuner, epochs, val_split, n_shifts):\n",
    "    x = train\n",
    "    y = np.roll(train, -1*n_shifts, axis=0)\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "    tuner.search(x, y,\n",
    "        epochs=epochs,\n",
    "        validation_split=val_split, \n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=7)])\n",
    "    \n",
    "    \n",
    "    best_model = tuner.get_best_models(num_models=1)\n",
    "    model = best_model=[0]\n",
    "\n",
    "    return model\n",
    "\n",
    "def y_predict(model, history, n_shifts):\n",
    "    import numpy as np\n",
    "    \n",
    "    # For if using differencing between train and test #\n",
    "    # prepare data\n",
    "    correction = 0.0\n",
    "    #if n_diff > 0:\n",
    "    #    correction = history[-n_diff]\n",
    "    #    history = difference(history, n_diff)\n",
    "    \n",
    "    # Make Predictions\n",
    "    x_input = np.array(history[-n_shifts:]).reshape((1, n_shifts, 1))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    return correction + yhat[0]\n",
    "\n",
    "# evaluate a single model\n",
    "def walk_forward_validation(data, tuner, n_shifts, epochs, val_split):\n",
    "    pred = list()\n",
    "    train, validation = train_test_split(data, test_size=val_split, shuffle=False)\n",
    "    history = [x for x in train]\n",
    "    print('Walk Forward Train', train.shape)\n",
    "    model = fit_model(train, tuner, epochs, val_split, n_shifts)\n",
    "    \n",
    "    # Loop through each time period in chronlogiccal order with past data\n",
    "    # Each period = n_shifts\n",
    "    for i in range(len(validation)):\n",
    "        yhat = y_predict(model, history, n_shifts) #Get prediction for y after n_shifts\n",
    "        pred.append(yhat) #Store predicted value\n",
    "        history.append(validaton[i]) #Add consecutive validation value to past data\n",
    "    \n",
    "    error = mse(validation, pred, squared=False) # Calculate overall error\n",
    "    #predictions = array(predictions)\n",
    "    #score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return error, pred\n",
    "\n",
    "def evaluate(data, test_split, val_split, n_shifts, epochs, repeats=1, hp_epochs=10):\n",
    "    #Set tuner \n",
    "    tuner = kt.Hyperband(model_builder,\n",
    "             objective='val_accuracy',\n",
    "             max_epochs=hp_epochs,\n",
    "             factor=3,\n",
    "             directory='C:\\\\Users\\\\noahd\\\\Google Drive\\\\University\\\\2k20-21\\\\Personal Project\\\\Data\\\\res',\n",
    "             project_name='Stock-CNN-LTSM')\n",
    "\n",
    "    \n",
    "    predictions_matrix = []\n",
    "    scores = []\n",
    "    train, test = train_test_split(data, test_size=test_split, shuffle=False) #Split to train and test\n",
    "    \n",
    "    for n in range(repeats):\n",
    "        score, predictions = walk_forward_validation(train, tuner, n_shifts, epochs, val_split)\n",
    "        scores.append(score)\n",
    "        predictions_matrix.append(predictions)\n",
    "    \n",
    "    return scores, prediction_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = evaluate(format_data_cnn(data_diff), 0.3, 0.3, 1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
