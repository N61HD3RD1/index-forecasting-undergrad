{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Project Research Notebook\n",
    "\n",
    "---\n",
    "## 1.0: Data Processing\n",
    "\n",
    "### 1.1: Retrieving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "//\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "//\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import urllib\n",
    "import datetime\n",
    "import csv\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from datetime import datetime as dt\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading csv files into pandas returns a dictionary with dataframes of tickers\n",
    "def load_to_df(tickers, path):\n",
    "    data = {}\n",
    "    \n",
    "    for i in range(len(tickers)):\n",
    "        full_path = path + \"/\" + tickers[i] + \".csv\"\n",
    "        ticker = tickers[i].replace('^', '')\n",
    "        data.update({ticker: pd.read_csv(full_path, parse_dates=True, index_col=0)})\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downlaoding Files to .csv\n",
    "path = \"C:\\\\Users\\\\noahd\\\\Google Drive\\\\University\\\\2k20-21\\\\Personal Project\\\\Data\\\\Stocks\"\n",
    "tickers = [\"^GSPC\", \"^DJI\", \"^FTSE\", \"^N225\", \"^BSESN\"]\n",
    "dates = ('2011-04-07', '2021-04-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = load_to_df(tickers, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Normalising and Differencing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#Applies normalisation and difference transfrom to a pandas dataframe\n",
    "#Can only normalise and remove Na if passed diff=False\n",
    "def process_data(data, dates, diff=True):\n",
    "    data_proc = data.copy()\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "    for key in data_proc:\n",
    "        df_copy = data_proc[key].copy(deep=True)# So the original data is not overwritten\n",
    "        df_copy = df_copy.reindex(pd.date_range(start=dates[0], end=dates[1], freq='D')) #For using mutiple time series, all trackers must have the same dates\n",
    "        \n",
    "        if diff==True:\n",
    "            df_copy = df_copy.diff() #Difference transform\n",
    "        \n",
    "        df_copy = df_copy.resample('d').mean().interpolate('spline', order=3, s=0) #Iterpolate missing dates\n",
    "        df_copy = pd.DataFrame(scaler.fit_transform(df_copy.values), columns = df_copy.columns, index=df_copy.index.values) #Scales values\n",
    "        df_copy = df_copy.drop(df_copy.tail(1).index)\n",
    "        df_copy = df_copy.dropna(axis=0)\n",
    "        data_proc[key] = df_copy # Assign new data back to data dictionary    \n",
    "    \n",
    "    return data_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Processing all Datasets\n",
    "data_normal = process_data(data_raw, dates, diff=False)\n",
    "data_diff = process_data(data_raw, dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3: Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: CNN-LSTM Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as kr\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "#Model Layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Sequential\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# --- Model Fitting Callbacks --- #\n",
    "modelfit_callbacks = [tf.keras.callbacks.TensorBoard(log_dir=path + \"\\\\logs\\\\fit\\\\\" + str(datetime.datetime), histogram_freq=1),\n",
    "                      tf.keras.callbacks.EarlyStopping('val_loss', patience=7)]\n",
    "\n",
    "# https://gist.github.com/GermanCM/1943a0dc1eac04f848c6fe9b16947ac4\n",
    "#Contains methods for building, trainig, optimising and validating the model.\n",
    "def reset_weights(model):\n",
    "    import keras.backend as K\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'): \n",
    "            layer.kernel.initializer.run(session=session)\n",
    "        if hasattr(layer, 'bias_initializer'):\n",
    "            layer.bias.initializer.run(session=session)  \n",
    "\n",
    "\n",
    "#Takes hyperpameter tuner as input reutns model, to be run with keras tuner\n",
    "def model_builder(hp, samples=1, time_steps=1, features=30, params='Egg'):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # --- Hypertuner Parameter Variables --- #\n",
    "    #Dictionary organising all params for easy reading\n",
    "    params = {\n",
    "        # Layer Parameters\n",
    "        'conv1d' : {\n",
    "            'units' : hp.Int('conv_units', min_value=1, max_value=8, step=1), #Conv1D Units\n",
    "            'kernal_size' : hp.Int('kernal_size', min_value=2, max_value=10, step=1),\n",
    "            'filters' : hp.Int('filters', min_value=4, max_value=16, step=1)\n",
    "        },\n",
    "        'max_pooling' : {\n",
    "            'pool_size' : hp.Int('pool_size', min_value=1, max_value=4, step=1)\n",
    "        },\n",
    "        'lstm' : {\n",
    "            'units' : hp.Int('lstm_units', min_value=5, max_value=255, step=5)\n",
    "        },\n",
    "        'dense' : {\n",
    "            'units' : hp.Int('dense_units', min_value=4, max_value=256, step=4)\n",
    "        },\n",
    "        # Hyper Parameters\n",
    "        'hp' : {\n",
    "            'lr' : hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    if(params=='Default'):\n",
    "        paramas = {\n",
    "        'conv1D' : {'units': 1, 'filters' : 7},\n",
    "        'maxpooling' : {'pool_size' : 2},\n",
    "        'ltsm': {'units': 45},\n",
    "        'dense': {'units': 128},\n",
    "        'hp': {'lr': 1e-2}\n",
    "    }\n",
    "    \n",
    "    # --- Build Model === #\n",
    "    # Add time distributed -wrapped CNN layers\n",
    "    model.add(Conv1D(filters=params['conv1d']['filters'],\n",
    "                     kernel_size=params['conv1d']['kernal_size'],\n",
    "                     activation='relu', \n",
    "                     padding='causal',))\n",
    "    #model.add(MaxPooling1D(pool_size=params['max_pooling']['pool_size'], padding='valid'))\n",
    "    #model.add(Flatten())\n",
    "    model.add(LSTM(params['lstm']['units'])) #LTSM Layer\n",
    "    model.add(Dense(params['dense']['units']))\n",
    "    print(model.summary(line_length=None, positions=None, print_fn=None))\n",
    "\n",
    "    #Compile Model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=params['hp']['lr']),\n",
    "        loss=kr.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "#Fits the model using keras tuner, hypertuner object. Returns the most model.\n",
    "def fit_model(train, tuner, epochs, val_split, n_shifts):\n",
    "    x = train\n",
    "    y = np.roll(train, -1*n_shifts, axis=0) #Lags the y input\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "    tuner.search(tf.keras.Input(shape=30), y,\n",
    "        epochs=epochs,\n",
    "        validation_split=val_split, \n",
    "        callbacks=modelfit_callbacks)\n",
    "    \n",
    "    \n",
    "    best_model = tuner.get_best_models(num_models=1)\n",
    "    model = best_model=[0]\n",
    "\n",
    "    return model\n",
    "\n",
    "def y_predict(model, history, n_shifts):\n",
    "    import numpy as np\n",
    "    \n",
    "    # For if using differencing between train and test #\n",
    "    # prepare data\n",
    "    correction = 0.0\n",
    "    #if n_diff > 0:\n",
    "    #    correction = history[-n_diff]\n",
    "    #    history = difference(history, n_diff)\n",
    "    \n",
    "    # Make Predictions\n",
    "    x_input = np.array(history[-n_shifts:]).reshape((1, n_shifts, 1))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    return correction + yhat[0]\n",
    "\n",
    "# evaluate a single model\n",
    "def walk_forward_validation(data, tuner, n_shifts, epochs, val_split):\n",
    "    pred = list()\n",
    "    train, validation = train_test_split(data, test_size=val_split, shuffle=False)\n",
    "    history = [x for x in train]\n",
    "    print('Walk Forward Train', train.shape)\n",
    "    model = fit_model(train, tuner, epochs, val_split, n_shifts)\n",
    "    \n",
    "    # Loop through each time period in chronlogiccal order with past data\n",
    "    # Each period = n_shifts\n",
    "    for i in range(len(validation)):\n",
    "        yhat = y_predict(model, history, n_shifts) #Get prediction for y after n_shifts\n",
    "        pred.append(yhat) #Store predicted value\n",
    "        history.append(validaton[i]) #Add consecutive validation value to past data\n",
    "    \n",
    "    error = mse(validation, pred, squared=False) # Calculate overall error\n",
    "    #predictions = array(predictions)\n",
    "    #score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return error, pred\n",
    "\n",
    "def evaluate(data, test_split, val_split, n_shifts, epochs, repeats=1, hp_epochs=10):\n",
    "    #Set tuner \n",
    "    tuner = kt.Hyperband(model_builder,\n",
    "             objective='val_accuracy',\n",
    "             max_epochs=hp_epochs,\n",
    "             factor=3,\n",
    "             directory='C:\\\\Users\\\\noahd\\\\Google Drive\\\\University\\\\2k20-21\\\\Personal Project\\\\Data\\\\res',\n",
    "             project_name='Stock-CNN-LTSM')\n",
    "\n",
    "    \n",
    "    predictions_matrix = []\n",
    "    scores = []\n",
    "    train, test = train_test_split(data, test_size=test_split, shuffle=False) #Split to train and test\n",
    "    \n",
    "    for n in range(repeats):\n",
    "        score, predictions = walk_forward_validation(train, tuner, n_shifts, epochs, val_split)\n",
    "        scores.append(score)\n",
    "        predictions_matrix.append(predictions)\n",
    "    \n",
    "    return scores, prediction_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "#Using tensorboard for network graphs and debugging\n",
    "%load_ext tensorboard\n",
    "import tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates input Matrix x(n,m,l): Where data is n; step_size, m: samples, l; features.\n",
    "# Steps is the period between each set of timestep. For steps=1, n[1] = n[2]\n",
    "# Sample size is the number of samples per timestep\n",
    "# Features is determined by the number features in the dataset, ie: Tackers*Tracker-Atrributes\n",
    "# Ouput vector y(a, b, c), where a is the timestep after last sample in m\n",
    "# b is the size of the forecast horizon, the number of values to predict, and c is the target values for prediction\n",
    "# Returns this data format, and a shifted version to use for rolling predition as y\n",
    "def format_data_cnn(data_dict, step_size, sample_size, forecast_horizon):\n",
    "    data_flat = pd.DataFrame()\n",
    "    for i, df in enumerate(data_dict.values()):\n",
    "        data_flat.concat(df, axis=1)\n",
    "    \n",
    "    #data = np.asarray(data)\n",
    "    #data  = np.swapaxes(data, 0, 1) #Get the data in right alignment\n",
    "    #data = np.reshape(data, (data.shape[0], (data.shape[1]*data.shape[2])), order='C') #Merge 2nd and 3d axis to make 2D array\n",
    "    print(data.shape)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'concat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-1ce1c21245ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_cnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_data_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_diff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-d5c100e69286>\u001b[0m in \u001b[0;36mformat_data_cnn\u001b[1;34m(data_dict, step_size, sample_size, forecast_horizon)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdata_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mdata_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#data = np.asarray(data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-2-3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5463\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5464\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5465\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5467\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'concat'"
     ]
    }
   ],
   "source": [
    "data_cnn = format_data_cnn(data_diff, 1, 60, 1)\n",
    "print(data_cnn.shape)\n",
    "test = evaluate(data_cnn, 0.3, 0.3, 1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
